---
title: "R Notebook"
output: html_notebook
---

#Data and libraries

```{r}
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting
library(rsample)   # for data splitting

# Modeling packages
library(caret)     # for logistic regression modeling

# Model interpretability packages
library(vip)       # variable importance
library(ROCR)

# Data
library(kernlab)
data(spam)
head(spam)
```
#split the data

```{r}
set.seed(123) # for reproducibility
split <- initial_split(spam, strata = "type", prop = 0.7)
spam_train <- training(split)
spam_test  <- testing(split)
```

#Quistion 1
*Pick a single feature and apply simple logistic regression model.

```{r}
spam_model1 <- glm(type ~ you, family = "binomial", data = spam_train)
spam_model1
```
*Interpret the feature’s coefficient
*What is the model’s performance?
```{r}
tidy(spam_model1)
exp(coef(spam_model1))
```

#Quistion 2
*Pick another feature to add to the model.
*Before applying the module why do you think this feature will help?
*Apply a logistic regression model with the two features and compare to the simple linear model.


```{r}
spam_model2 <- glm(type ~ you + num415, family = "binomial", data = spam_train)
spam_model2
```
*Interpret the coefficients.
```{r}
tidy(spam_model2)
exp(coef(spam_model2))
```

#Quistion 3
*Now apply a model that includes all the predictors.
*How does this model compare to the previous two?

```{r}
spam_model3 <- glm(type ~ ., family = "binomial", data = spam_train)
spam_model3
```
 
#Quistion 4
 *Plot an ROC curve comparing the performance of all three models
 
```{r}


set.seed(123)
cv_model1 <- train(
  type ~ you , 
  data = spam_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model2 <- train(
  type ~ you + num415, 
  data = spam_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
set.seed(123)
cv_model3 <- train(
  type ~ ., 
  data = spam_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
# extract out of sample performance measures
summary(
  resamples(
    list(
      spam_model1 = cv_model1, 
      spam_model2 = cv_model2, 
      spam_model3 = cv_model3
    )
  )
)$statistics$Accuracy
```
 *ROS
 
```{r}
# Compute predicted probabilities
m1_prob <- predict(cv_model1, spam_train, type = "prob")$spam
m3_prob <- predict(cv_model3, spam_train, type = "prob")$spam


# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, spam_train$type) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m3_prob, spam_train$type) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
legend(0.8, 0.2, legend = c("cv_model1", "cv_model3"),
       col = c("black", "blue"), lty = 2:1, cex = 0.6)
```
#Quistion 5
Compute and interpret the following performance metrics:
No information rate
accuracy rate
sensitivity
specificity

```{r}
# predict class
pred_class <- predict(cv_model3, spam_train)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class, ref = "spam"), 
  reference = relevel(spam_train$type, ref = "spam")
)
```

